{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordNet\n",
    "* semantički orijentirani riječnik engleskog jezika, sličan tezarusu\n",
    "* 155,287 riječi i 117,659 sinonimskih skupova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Smisao i sinonimi\n",
    "* __sinonimi__ = riječi istog značenja, npr. hrv. ljekarna - apoteka, engl. motorcar - automobile\n",
    "* WordNet korpus u NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.02.car'),\n",
       " Lemma('car.n.03.car'),\n",
       " Lemma('car.n.04.car'),\n",
       " Lemma('cable_car.n.01.car')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('wolves')\n",
    "wordnet.synset('dog.n.01').lemma_names() # leme sinonima\n",
    "# definicija\n",
    "wordnet.synset('dog.n.01').definition() # defincija sinonimskog skupa \n",
    "# primjer\n",
    "wordnet.synset('dog.n.01').examples() # primjer koristenja\n",
    "# leme\n",
    "wordnet.synset('car.n.01').lemmas()   # leme pripadnih sinonima\n",
    "wordnet.lemma('car.n.01.automobile').synset() \n",
    "wordnet.lemma('car.n.01.automobile').name()  # naziv leme\n",
    "\n",
    "\n",
    "# sinonimski skupovi\n",
    "wordnet.synsets('car') # nekoliko ....\n",
    "wordnet.lemmas('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordNet hijerarhija\n",
    "* Sinonimski skupovi WordNeta se povezuju s apstraktnim konceptima opći(`Entity`,`State`,`Event`) i specijalizirani koncepti (npr. hatchback tip vozila )\n",
    "\n",
    "<img src=\"wordnet.png\" width=\"650\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Leksičke relacije\n",
    "\n",
    "* preko takvih koncepata možemo doći do *leksičkih relacija*:\n",
    "    * __hiponimi/hipernimi__ = pojmovi koji predstavljaju jezični analogon podskupa/nadskupa nekog pojma, u hijerarhijskom smislu mogu se predstaviti u relaciji _roditelj-dijete_\n",
    "    *  __meronimi__ = pojmovi koji predstavljaju komponente od kojih je načinjen neki drugi pojam\n",
    "    * __holonimi__  = pojmovi unutar kojeg je sadržan neki pojam\n",
    "    * __antonimi__  = pojmovi suprotnog značenja\n",
    "    * __inherentni pojmovi (engl. entailment)__  = pojmovi koji su inherentno uključeni u dani pojam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hiponimi: [Synset('cab.n.03'), Synset('racer.n.02'), Synset('hardtop.n.01'), Synset('minivan.n.01'), Synset('limousine.n.01'), Synset('used-car.n.01'), Synset('bus.n.04'), Synset('sport_utility.n.01'), Synset('horseless_carriage.n.01'), Synset('ambulance.n.01'), Synset('roadster.n.01'), Synset('convertible.n.01'), Synset('gas_guzzler.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('beach_wagon.n.01'), Synset('coupe.n.01'), Synset('pace_car.n.01'), Synset('stanley_steamer.n.01'), Synset('electric.n.01'), Synset('jeep.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('compact.n.03'), Synset('hot_rod.n.01'), Synset('cruiser.n.01'), Synset('hatchback.n.01'), Synset('sedan.n.01'), Synset('sports_car.n.01'), Synset('stock_car.n.01'), Synset('model_t.n.01')]\n",
      "\n",
      "hipernimi: [Synset('motor_vehicle.n.01')]\n",
      "\n",
      "skupovi hipernima na putu 1: ['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01', 'instrumentality.n.03', 'container.n.01', 'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n",
      "skupovi hipernima na putu 2: ['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01', 'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01', 'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiponimi i hipernimi\n",
    "from nltk.corpus import wordnet\n",
    "motorcar = wordnet.synset('car.n.01')\n",
    "# hiponimi\n",
    "hypoMotorcar = motorcar.hyponyms() # synset hiponima\n",
    "print('\\nhiponimi:', hypoMotorcar)\n",
    "#sorted([lemma.name() for synset in hypoMotorcar for lemma in synset.lemmas()]) # ispisi nazive lema\n",
    "# hipernimi\n",
    "hyperMotorcar = motorcar.hypernyms()\n",
    "print('\\nhipernimi:', hyperMotorcar)\n",
    "# putanja car -> entity\n",
    "paths = motorcar.hypernym_paths()\n",
    "# popis sinonimskih skupova za prvi put\n",
    "\n",
    "print('\\nskupovi hipernima na putu 1:', [synset.name() for synset in paths[0]])\n",
    "# popis sinonimskih skupova za drugi put\n",
    "print('skupovi hipernima na putu 2:', [synset.name() for synset in paths[1]])\n",
    "\n",
    "# korijenski hipernim:\n",
    "motorcar.root_hypernyms()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('vertical.a.01.vertical'), Lemma('inclined.a.02.inclined')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meronimi i holonimi\n",
    "from nltk.corpus import wordnet\n",
    "tree = wordnet.synset('tree.n.01')\n",
    "# pojmovi kao dijelovi dreveta\n",
    "tree.part_meronyms()\n",
    "# pojmovi kao srz drveta\n",
    "tree.substance_meronyms()\n",
    "# holonim\n",
    "tree.member_holonyms()\n",
    "\n",
    "# inherentni pojmovi\n",
    "wordnet.synset('walk.v.01').entailments()\n",
    "\n",
    "# antonimi\n",
    "wordnet.synsets('supply')\n",
    "wordnet.synset('supply.n.02').lemmas()\n",
    "wordnet.lemma('supply.n.02.supply').antonyms()\n",
    "\n",
    "\n",
    "wordnet.lemma('horizontal.a.01.horizontal').antonyms()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semantička sličnost\n",
    "* semantički skupovi kao vrhovi stabla u kojima bridovi čine leksičke relacije\n",
    "* za dani semantički skup, obilazak stabla predstavlja pronalaženje semantičkih skupova sličnog značenja\n",
    "* **primjena**: optimizacija indeksiranja kolekcije tekstova\n",
    "    \n",
    "__Ideja__: 2 semantička skupa $S_1,S_2$ su povezani korijenskim semantičkim skupom $r$. Putovi od $S_1$ i $S_2$ mogu imati zajedničke vrhove: \n",
    "> __Problem najnižeg zajedničkog hipernima__ = LCA problem na stablu!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# LCH od 'mother' 'kin'?\n",
    "wordnet.synset('kin.n.01').lowest_common_hypernyms(wordnet.synset('mother.n.01'))\n",
    "# LCH  od 'policeman' , 'chef'\n",
    "wordnet.synset('policeman.n.01').lowest_common_hypernyms(wordnet.synset('chef.n.01'))\n",
    "# mjera sličnosti pojmova\n",
    "wordnet.synset('kin.n.01').path_similarity(wordnet.synset('mother.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vektori riječi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Domagoj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30103"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_kjv_sents = gutenberg.sents('bible-kjv.txt')\n",
    "len(bible_kjv_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'god',\n",
       " 'created',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# priprema podataka\n",
    "\n",
    "# oznake interpunkcije\n",
    "from string import punctuation\n",
    "# čišćenje i normalizacija podataka\n",
    "dataset = [[word.lower() for word in sent if word not in punctuation and word.isalpha()] \n",
    "                                            for sent in bible_kjv_sents]\n",
    "# primjer nekih\n",
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=dataset, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "\n",
    "# Get the vocabulary dictionary\n",
    "vocab_dict = model.wv.key_to_index\n",
    "\n",
    "# word vectors\n",
    "word_vectors = model.wv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7853802 , -1.1370678 , -1.757116  ,  0.7734806 , -0.9040016 ,\n",
       "        1.8818294 ,  0.73379695, -1.5174632 ,  0.32443914,  0.23261325,\n",
       "        0.7250204 , -0.69703436, -0.00785978, -1.0855719 , -1.3795278 ,\n",
       "        0.2933443 , -0.05955132, -1.2506769 ,  1.9209949 , -0.82333595,\n",
       "        0.9007139 ,  0.0212526 ,  0.2966663 ,  1.1132697 , -0.36178654,\n",
       "       -0.26940352,  0.5864987 , -0.4433881 , -0.13097814,  0.03077465,\n",
       "       -0.5201972 ,  1.4347211 , -0.20105758,  0.97355723, -0.53479517,\n",
       "       -0.5044984 , -1.5228078 ,  0.04923371,  0.4798079 , -0.25119725,\n",
       "       -1.2426051 ,  0.64930826,  1.8503653 , -0.5877428 , -0.8388758 ,\n",
       "       -0.96825314, -0.5493941 , -0.15711442,  1.6777301 , -0.6632398 ,\n",
       "       -0.2579473 ,  1.5833191 , -0.5235548 , -0.11268839, -0.2848162 ,\n",
       "       -2.0332131 ,  1.7460198 , -0.09777386, -1.4304943 ,  0.5327749 ,\n",
       "        0.12688543, -0.23293008, -0.49488825, -1.3036995 , -2.0768068 ,\n",
       "       -1.1438383 , -0.68346566,  0.3856971 , -0.15492526,  0.66124153,\n",
       "        0.17935407, -0.31245673, -0.63584304,  0.8938265 ,  2.0978796 ,\n",
       "        0.5124129 ,  0.6405297 ,  0.7385866 ,  0.62408036,  0.26886392,\n",
       "       -0.68930227,  0.59515214,  0.26492783,  1.0077398 , -0.48136127,\n",
       "       -1.4320772 , -0.05981417,  1.9291024 , -0.07685683, -0.84089303,\n",
       "        1.1852146 ,  1.9030347 ,  0.8127694 , -0.34043637,  0.29484913,\n",
       "       -0.61699754,  2.3584304 , -1.1015229 ,  0.45626026,  1.0325752 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"god\"\n",
    "# vektor riječi za \"god\"\n",
    "v_god = word_vectors[word]\n",
    "\n",
    "# koliko puta se pojavila rijec \"god\"\n",
    "model.wv.get_vecattr(word, \"count\")\n",
    "\n",
    "v_god"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lord', 0.7382637858390808),\n",
       " ('spirit', 0.7180866599082947),\n",
       " ('truth', 0.7147852182388306),\n",
       " ('christ', 0.7022433280944824),\n",
       " ('faith', 0.6983363628387451),\n",
       " ('hosts', 0.6967219114303589),\n",
       " ('glory', 0.691231369972229),\n",
       " ('salvation', 0.6855317950248718),\n",
       " ('gospel', 0.6765190362930298),\n",
       " ('grace', 0.6386775970458984)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most similar words to \"god\"\n",
    "most_similar_words = model.wv.most_similar(\"god\")\n",
    "\n",
    "most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6350624561309814)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'food'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.doesnt_match(\"lord god salvation food spirit\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>%{hovertext}</b><br><br>x=%{x}<br>y=%{y}<br>z=%{z}<extra></extra>",
         "hovertext": [
          "the",
          "and",
          "of",
          "to",
          "that",
          "in",
          "he",
          "shall",
          "unto",
          "for",
          "i",
          "his",
          "a",
          "lord",
          "they",
          "be",
          "is",
          "him",
          "not",
          "them",
          "it",
          "with",
          "all",
          "thou",
          "thy",
          "was",
          "god",
          "which",
          "my",
          "me",
          "said",
          "but",
          "ye",
          "their",
          "have",
          "will",
          "thee",
          "from",
          "as",
          "are",
          "when",
          "this",
          "out",
          "were",
          "upon",
          "man",
          "by",
          "you",
          "israel",
          "king",
          "son",
          "up",
          "there",
          "hath",
          "then",
          "people",
          "came",
          "had",
          "house",
          "into",
          "on",
          "her",
          "come",
          "one",
          "we",
          "children",
          "before",
          "s",
          "your",
          "also",
          "day",
          "land",
          "an",
          "so",
          "men",
          "against",
          "shalt",
          "if",
          "at",
          "let",
          "go",
          "hand",
          "us",
          "saying",
          "made",
          "went",
          "no",
          "even",
          "do",
          "now",
          "behold",
          "saith",
          "therefore",
          "every",
          "these",
          "because",
          "after",
          "our",
          "things",
          "father",
          "down",
          "or",
          "sons",
          "hast",
          "o",
          "david",
          "say",
          "make",
          "may",
          "over",
          "did",
          "earth",
          "what",
          "jesus",
          "she",
          "who",
          "great",
          "name",
          "thine",
          "among",
          "away",
          "any",
          "put",
          "thereof",
          "forth",
          "give",
          "neither",
          "am",
          "take",
          "city",
          "days",
          "brought",
          "moses",
          "two",
          "heart",
          "pass",
          "judah",
          "jerusalem",
          "according",
          "should",
          "know",
          "whom",
          "nor",
          "took",
          "thus",
          "bring",
          "offering",
          "good",
          "place",
          "word",
          "set",
          "more",
          "sent",
          "yet",
          "again",
          "like",
          "way",
          "eat",
          "mine",
          "heard",
          "called",
          "about",
          "time",
          "evil",
          "holy",
          "egypt",
          "see",
          "own",
          "hundred",
          "spake",
          "heaven",
          "christ",
          "done",
          "brethren",
          "many",
          "hear",
          "fire",
          "fathers",
          "saw",
          "words",
          "priest",
          "how",
          "thing",
          "years",
          "himself",
          "law",
          "thousand",
          "speak",
          "off",
          "voice",
          "spirit",
          "eyes",
          "cast",
          "servant",
          "given",
          "art",
          "answered",
          "three",
          "together",
          "than",
          "servants",
          "ever",
          "might",
          "those",
          "gave",
          "other",
          "through",
          "seven",
          "hands",
          "soul",
          "another",
          "would",
          "life",
          "cities",
          "blood",
          "sin",
          "first",
          "commanded",
          "side",
          "peace",
          "without",
          "sword",
          "mouth",
          "saul",
          "flesh",
          "work",
          "gold",
          "high",
          "face",
          "themselves",
          "wife",
          "glory",
          "brother",
          "found",
          "where",
          "priests",
          "fear",
          "sea",
          "water",
          "under",
          "old",
          "altar",
          "jacob",
          "death",
          "drink",
          "year",
          "burnt",
          "until",
          "congregation",
          "dead",
          "head",
          "woman",
          "midst",
          "keep",
          "bread",
          "both",
          "right",
          "none",
          "aaron",
          "left",
          "wherefore",
          "toward",
          "five",
          "wicked",
          "kingdom",
          "kings",
          "yea",
          "stood",
          "dwell",
          "taken",
          "nations",
          "sight",
          "same",
          "been",
          "tabernacle",
          "cause",
          "four",
          "daughter",
          "die",
          "round",
          "silver",
          "cut",
          "mother",
          "whose",
          "pray",
          "love",
          "end",
          "night",
          "righteousness",
          "solomon",
          "wilderness",
          "blessed",
          "young",
          "hosts",
          "deliver",
          "babylon",
          "judgment",
          "twenty",
          "covenant",
          "being"
         ],
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "size": 3,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "type": "scatter3d",
         "x": {
          "bdata": "qzKvv+UX9L8/JOK/8tRNP1ndCEANZni/VH31Pnl67j/g2Xg/47L0P0VOhUDDRde+NCIXv2CLkUCl23o96reaPxjY+j8++dQ/OKyQQPftqj8qGIE/xbi+v/44Sr4sfZFAwKOLQDtLkMCUYZ9AB6AMP5EMokA315lAsnAbQAC3N0AQ2ZJAdNT6viEqKkAq96JAhzeqQASdk78Nddo9b+vGPuyXs765xHNAK/gQwBBYysDLZyLAkZXsP0ogkb9M2rFA3+hTPxMYhL9wGRDAGeD8vysIlr+RRQxAYjSHPg6mrz/gwovA/eOEwJ6/hL9M3C3AijdPwDuY2r5K39c/AH/4vwRogECOMvu+hYOFv0kQmr/ZxXNAiTpsPlhKzD5bFze9CnWpv/BRFb2fDaq/lNSHPekbuD9+spRA6qsMwPv7S0BX78c/+A0BP3qnhkB0wSJA+zsdwHeCusCJ8UJAhF+zv5SMuUBHwEBAlIAuP56PRUAgUm9AqYHIv61pFz+++A9Af67/v6iuUkB/eidAuRYUQN7LM8BqNKU/Lu1vwM9pBUDYe6VALMG3vk2Qe0C2cQM/wKtxQHVJXsCgTcq+xVMEvzaLmUCaILU/nCc5v7ri0D+yZqy/CQoHQIdUgEAgPme+h32ivuj7SEBfE3a/6HONwIk+4b9YhgNA+LJEQFvJbkADIqE+iswiwKyS5r83zCrA0iIXv2IH0MBkEFdADgXlv3JbT8A+s++/WLSBvhAgNEBfEIJAZ1DtP/KxF0AkMLXA1jo3QI0HxL33OQTAmP9uQMhfVr/RSxpAKZgDwJAlDUCz0T2/d876P7rdFD/gWhvAGLxIvtl6H76nw2RAQQuNPMlFkL/xdbHAXnx4vlxmSkAuZH0/Yo7sv+NCI0A83IU/sc0Cwde3Wb5qgiK/oSQyQHevPUCwIAW+Va9AvpLBhUANniTAcCjqPj5CXcAXzjJAvjMSwBaxOEDAmzFAvXXDwD8clz6aExFAYN3nwC94P0D4eZO/FaIDQEzryD/uoNQ/YHAHwI36HUDCNY0/iZ3VP05nsz+dnarAs3MswMcTtj+J200/fMhGQJWgA0AjqyG/JlYrwM5lGMAm/x2/EJiQwIjISL9Ej0NAqpkov7NMSEAicxJAflKewGvTDb+dWq8/59IdwJTWwT5/WtXAG9zgP7/Qwb/n/aO/8eF+Pw7cfb8eOdg+y3OMvKfRqcAJombAI4nQvt1Urr/qKeO+jHyNP3B6JLxHusO8SPllv+gYYsASlBNA+LiKwLu8zL83c0PApQM7wBqYWsDhDCo9kgZpP2Vbwz68dq/AXOf8v+KTnr/rkcy/NFkBP1q2IMCt8SU9pLIpwG0iIkDIMke+2729vyKtMz6Io5M/THD1v0axz7/J4QhAsulTwEai2sB33LU/EjKQPnqfG8DmL84/oIGKwDhuHDwiJcO/eAssv6zWoj+IdFi/de59Pyu2P8AEAdM/3wjDwBYxAb/OL+Q/cEijwAIspsAhXdK/FWdPv2bUi78mQXtAcvYOQK3Wt7/I6XG/mBghQFDgy7/yBivA+aKNP2jyH8BfrsI/kCOxPyugQMCI3rg/+0HfwAa4Rz8IQUy/",
          "dtype": "f4"
         },
         "y": {
          "bdata": "0Qx5PrmM5L57BR6/AQqcvc/M4T3hDq4/n/OyvmMTjUDTbirA3IS4P5pIbz78+98/a48vQAOBN8AvYMs/7Y0LQAgdiD8q67y/wqKRP7XTqD5LZBdAejzmP6YvwL2xSDk/LM8jQBm5AMBrLxnAdz6bv5gYAUDI7B2/9uXAwM1N4z2dl70/2x+JQBGuzL3IuzpAqcxKP/HdHj+vm/A/BBr2P0IJ2L+eABa/fWB6P8w8YL/UGYNAgHVKv6hd3j7ryBI/YsYuwC9e/8DWAg3BsOZZP1pGqz/M0Qa//79bwCe4GMAI6LHAau46wBX0KsAH3KQ/87YwQE2eFEDNYoy/ROdNP6PcIT83PoXA2zaGvdyCfL8BU5NAn4BPv20Qwr/eF2K++UUZQMP5g7+mdY+/mI+qvwmzgUD1QARAC6VUPu+h2j8rIyC/Y/dOv68pIz45WL3AgJIPP2I4jMBZ03pA7OMovmFDFD8TUgXAuFOAv2cHMMAdpDO/OcVPQHzCpb9vm8K772a6vg7yqz8yNqK9n1F4wDy9M0CKnZZAI4KdwOgIFr8DTBvAehiJwN0jBsBe0B5AX/57QMO7Ub8hOmK/5SkfQEAe0r95PpfAgKOuvHqnCsBPNHc/fJhtwAP5lUCoTTo/WGrdP62JkEBKBhZAUamSQDcAkj/nymU/RjV/QGJOgL9ZwMI/10yCv8KnPb/arci/HMCvwGal4D/3nkY/aljNvze7dsB1RwzApaH/v8SymT8/n7m+BqwuwPTGlUCNB8u/eHyTwANqOL7PLPA/PcUgQCYvE74W3qfAlo4hP6UIIUDKpZ3AhhyGP/L/y79v/kBAYTD9PP4OekD7DVRAWbNOwDBCTMDs8dI/tztqv7Rp6D/MLxQ/Wcmsv9hKyD8FKENAkrGDP2VOycCxNXc/ZMhSwOvWkL9VvRrAcw6LP1uV9L/yvStAqkMqwLINBMDwl2bAjitPwMGyIb8K/4s/l8rmvq/5fr5pICzAJa+AP94HCsAaNn9AEf1UwBOhhb9uer8/WJvNPxuLgsDtHvS8yp7UvyqqgcAdVoc/q4KRPSwUjkAnkRLAlMxOP0ACuT/QVbQ96Vz+v3fgAEBEKKQ/eWMZQHz5jz+sDeI+BntDPxu3gj7rn6s/PfahPUWgRUBqQ5U/Vnylv5z8QMAo9s0+R5CtP3lPV0BBEro/41xGvDfVQsCwQR1AwoHnPxwAkUCikVs+GmEHPkL3dz++kirAiv+Pvx4/GsAMd4E/Ex94P/F5ScCvA8a+yo07Pz/vLkB2Dl5Aff3ePVgFZT+hmlnAU1vcP4Lod0BQuDDAz1ggQOAhCEA9LRLAV5eWvscKpD+FfjS/C18QP+YbNj7Sf09ArIuMP5CZLkDuFAJAnuw4wCpjmj8V5JW/2/pbP/au6T8yXv8/BTSuv6wn1L8PmxFAPCz5v4QCwz9vrU0/mxxcPihldT6cjfu+Iq1BPvXCwb/ohIE/mIr9P8CYW8BLss0/yz8zQHmKRUBNiIJAWAcOwHF5wD9dLZI+EZ4sP682Zz5slS8/NYotPzu1DMCpzqw9cS6Yv02Qcz8E2SzARxCJP1xREcBhzuQ+xxhnP//nu7/mfRM9",
          "dtype": "f4"
         },
         "z": {
          "bdata": "y5AtvzEumz+wDB2/OcjGPptuNjyL7c6+aqgHQFCGkb/S1+e9837EvZJ3oL4i4X5AM0yFQA0ejL+0MDTAiavov4Sw0T4otxpAt9Ikvijg+L8QxIY/2zrEP9IYd8CcL2A/a2YAQNiD4T7rthy+nbyav6GzGECd/HE/RucdQDPPBj6uOZvAK/hCwMmcoMAnpau+MAdYP+spA74+mwa+wMXKwLA7Sz8zkYq/+P34P8nbvsAc4zxAeZxkQEnnTr+48W3Akn9MwOQwST/23sRA6NEnQFp5777bU5g+eEXtP2+WOsA1mQnA8VpRwITEkz+WARxA5cDdP9+LhkDJSdC/7yYqP5YXKsA2PWTAF97LvgN/xUBlSY3AmFCUPgHJ1b8fCnHAmpIrQMu1Hb4SgznAuG5nv2XCxj9uAARAcQQQv8iASz8uC76/bHeFQEFzUr/SOZM/Ns5Dv+9DUr8GsHc/oVCuvzFSacC8BDW/PRrUP2aT5r+Pz/G+VgYjP2GWmMDtJVW+tXOvv/eCA8DeEoXAFzJgQPAwd0CT3UFAX7usP0nMdz3ZVQu+i2i6P/Al4b8ZhY6/mGjnvxCIFL+VmB/AOE+Wv8WdJz6LscY/zWSoQPyyDEASsSK/dLUGQDJWVEApPFLAf7LrP5GzZbywj8I/0qCmvxuhR0DSYMu+TDp+PgSxs78P8as/WvgPvZUCPMA2xu29FkHkvlVCOT7/pLc/PmoCwOPFDcBcWgnAtZUtwMYdCMB4tyXAic4mPwAvmr4pAPc/vYGnv2DaE79Ej+4/Vh7EPD2Ilz70FMm+P+rIPRAgtb9iB84+XN6APW4Jiz8DBpE/Uv4/P76plL8Hdeg/XVTdv7cM0z9ISADA0yW5v1mZjL9F/qy/bHLQv8oJYr+4XxVA2wOWvwsDx78P778+mZsAP8UZ9L+ZFhi/unVDwMRcxr/+/Q4/uVgbwBngu76bsdW/7Y2MQNpu+76an6o+h+7Pv3EkEkDjgHa+qdL9v3Q/6b+8/DdAw2zAP2jpjz+IhJ0/qgfhPh72PkCK3ec+6uj8PyOiIj8Mrr+/gWk3wLZ6Ab+mv4c+OMQrP+H30b/+53zAx6chvWUU6L+KX5W/i4D5vzCXSz930w9AQaGsP6kCcL+XHMU/YepxwOeyxT/mpSc/+CmcvrNsv7+ClGS/IhO7PpBMYT+Yhoc/cHsaQLmFFUAu4BY/nMObv63Fsz/EyRY/wjwQQJ+qIcC5grVAVAZAP1+imECptok+XbNmP4gQH8DsFSC/k9Qmv/OEtD+41qc/N3dQvMSKRj9kqKA/s/vIPwNs1z7Uy3q+yu/UPi+sPb8RJd2/lpe8P0DuhkCgkFxAmWBGvnUso78xfeG+URb2vnDXU0Db7jO+L0KLP8s0Lz8fB1I+WbclP7SWM7/I9iM/EOX1PvpQNsBb3xS/ifnLvmkAQMBZf7I/03FkwLkb874Tb649TKitvqAvdL9n6pS97Fmjv5QpikDFywI/DpkMwEVWhz9Fi1k/qdGeQLxeNUCjCLg+YjaNPEacjb98pGO+dmFIP1JRWT+9e6e/YfU5P4turD9eVze/3oZqPn/kpj+CJyc+D2x+v2PI871mnis+",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 900,
        "legend": {
         "tracegroupgap": 0
        },
        "scene": {
         "bgcolor": "rgba(0,0,0,0)",
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "showbackground": false,
          "showgrid": false,
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "showbackground": false,
          "showgrid": false,
          "title": {
           "text": "y"
          }
         },
         "zaxis": {
          "showbackground": false,
          "showgrid": false,
          "title": {
           "text": "z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Word2Vec Visualization 3D"
        },
        "width": 1200
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "vectors = np.array([model.wv[word] for word in list(model.wv.index_to_key)[:300]])\n",
    "pca = PCA(n_components=3)\n",
    "reduced = pca.fit_transform(vectors)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': reduced[:, 0],\n",
    "    'y': reduced[:, 1],\n",
    "    'z': reduced[:, 2],\n",
    "    'word': list(model.wv.index_to_key)[:300]\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z', hover_name='word', title='Word2Vec Visualization 3D')\n",
    "fig.update_layout(\n",
    "    width=1200, \n",
    "    height=900,\n",
    "    scene=dict(\n",
    "        bgcolor='rgba(0,0,0,0)',\n",
    "        xaxis=dict(showgrid=False, showbackground=False),\n",
    "        yaxis=dict(showgrid=False, showbackground=False),\n",
    "        zaxis=dict(showgrid=False, showbackground=False)\n",
    "    )\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1.  Objasnite razliku između nadziranog i nenadziranog učenja u obradi prirodnog jezika.\n",
    "Nadzirano učenje u obradi prirodnog jezika temelji se na učenju iz označenih podataka, pri čemu model dobiva ulazne primjere zajedno s točnim izlazima (npr. rečenice označene vrstom sentimenta ili riječima pridruženim gramatičkim oznakama). Cilj je naučiti preslikavanje između ulaza i izlaza kako bi se mogla predvidjeti oznaka za nove, neviđene podatke.\n",
    "Nenadzirano učenje koristi neoznačene podatke, bez unaprijed zadanih točnih odgovora, te model samostalno otkriva obrasce i strukture u podacima, kao što su grupiranje sličnih dokumenata ili otkrivanje tema u tekstu.\n",
    "\n",
    "2.  Pretpostavimo da imamo binarni klasifikator za sentiment analizu temeljen na naivnom Bayesovom modelu. Kako se računa vjerojatnost klase za dokument koristeći? Napišite formulu.\n",
    " Neka je:\n",
    "\td dokument,\n",
    "\tc∈C={+,-}  klasa (pozitivan / negativan sentiment),\n",
    "\tdokument dpredstavljen nizom riječi w_1,w_2,…,w_(∣d∣)(bag-of-words).\n",
    "Prema Bayesovu pravilu i naivnoj Bayesovoj pretpostavci vrijedi:\n",
    "c ̂_NB=arg⁡(max⁡)┬(c∈C) P(c∣d)=arg⁡(max⁡)┬(c∈C) P(c)∏_(i=1)^(∣d∣)▒〖P(〗 w_i∣c)\n",
    "\n",
    "Radi numeričke stabilnosti, u praksi se koristi logaritamski oblik:\n",
    "c ̂_NB=arg⁡(max⁡)┬(c∈C) (log⁡P(c)+∑_(i=1)^(∣d∣)▒log⁡P(w_i∣c))\n",
    "\n",
    "\n",
    "3.  Kako se u binarnoj logističkoj regresiji modelira vjerojatnost da uzorak pripada nekoj klasi? Navedite formulu i objasnite ulogu sigmoid funkcije.\n",
    "U binarnoj logističkoj regresiji vjerojatnost da uzorak x pripada klasi y=1 modelira se kao\n",
    "P(y=1∣x)=σ(w^⊤ x+b)=1/(1+e^(-(w^⊤ x+b)) ),\n",
    "\n",
    "gdje su wvektor težina, bslobodni član, a σ(⋅)sigmoid funkcija.\n",
    "Uloga sigmoid funkcije je preslikati linearni izraz w^⊤ x+b u interval (0ⓜ,1), tako da se izlaz može tumačiti kao vjerojatnost pripadnosti klasi. Vjerojatnost druge klase dobiva se kao P(y=0∣x)=1-P(y=1∣x).\n",
    "\n",
    "4.  Koji je osnovni cilj K-means algoritma i kako se iterativno ažuriraju centri klastera?\n",
    " Osnovni cilj K-means algoritma je podijeliti skup podataka u Kklastera tako da su podaci unutar istog klastera što sličniji, a podaci u različitim klasterima što različitiji, odnosno da se minimizira suma kvadrata udaljenosti podataka od pripadnih centara klastera.\n",
    "Iterativno ažuriranje odvija se u dva koraka:\n",
    "\tDodjela klastera – svaki podatak pridružuje se najbližem centru klastera (najčešće prema euklidskoj udaljenosti).\n",
    "\tAžuriranje centara – svaki centar klastera računa se kao aritmetička sredina svih podataka dodijeljenih tom klasteru.\n",
    "Postupak se ponavlja dok se centri klastera više ne mijenjaju ili dok se ne postigne uvjet konvergencije.\n",
    "\n",
    "5.  Definirajte metrike preciznost i odziv u kontekstu evaluacije binarnog klasifikatora. Zašto se koristi F-mjera?\n",
    " U evaluaciji binarnog klasifikatora preciznost (precision) i odziv (recall) definiraju se pomoću elemenata matrice zabune:\n",
    "\tPreciznost je udio ispravno predviđenih pozitivnih primjera među svim primjerima koje je model označio kao pozitivne:\n",
    "\"Precision\"=TP/(TP+FP).\n",
    "\n",
    "\tOdziv je udio ispravno predviđenih pozitivnih primjera među svim stvarno pozitivnim primjerima:\n",
    "\"Recall\"=TP/(TP+FN).\n",
    "\n",
    "F-mjera (najčešće F_1) koristi se jer predstavlja harmonijsku sredinu preciznosti i odziva:\n",
    "F_1=(2⋅\"T\" P)/(\"2\" ⋅TP+FP+FN),\n",
    "\n",
    "te daje uravnoteženu mjeru uspješnosti klasifikatora, osobito kada postoji neravnoteža između klasa ili kada je važno istovremeno uzeti u obzir i preciznost i odziv.\n",
    "\n",
    "6. Objasnite što je WordNet baza podataka. Na koji način se iskazuje sličnost između pojmova preko WordNet-a?\n",
    "WordNet je leksičko-semantička baza podataka u kojoj su riječi grupirane u sinonimske skupove (synsete), pri čemu svaki skup predstavlja jedno značenje riječi. Između synseta su definirane različite leksičko-semantičke relacije, poput sinonimije, hiperonimije/hiponimije, antonimije i meronimije.\n",
    "Sličnost između pojmova u WordNet-u iskazuje se na temelju njihove pozicije u hijerarhijskoj strukturi (taksonomiji), primjerice mjerenjem duljine puta između synseta, dubine njihova zajedničkog nadređenog pojma (najnižeg zajedničkog hiperonima) ili kombinacijom tih kriterija.\n",
    "\n",
    "7.  Zašto se u modernom NLP-u preferira korištenje vektora riječi umjesto tradicionalnih one-hot reprezentacija? Objasnite prednosti ovog pristupa.\n",
    "U modernom NLP-u preferira se korištenje vektora riječi (word embeddings) umjesto one-hot reprezentacija jer vektori riječi omogućuju kompaktnu i semantički bogatu reprezentaciju značenja riječi.\n",
    "Za razliku od one-hot vektora, koji su vrlo visokodimenzionalni i rijetki te ne nose informaciju o sličnosti između riječi, vektori riječi su niskodimenzionalni, gusti i kodiraju semantičku i sintaktičku sličnost (npr. slične riječi imaju slične vektore). Time se omogućuje mjerenje sličnosti između riječi, bolja generalizacija modela te učinkovitije učenje i primjena u NLP zadacima.\n",
    "\n",
    "8.  Koja je temeljna ideja algoritma Word2Vec i koje su njegove dvije glavne varijante? Opišite kako se vektori riječi uče u ovom okviru.\n",
    "Temeljna ideja Word2Vec algoritma jest učiti vektorske reprezentacije riječi tako da riječi koje se pojavljuju u sličnim kontekstima imaju slične vektore, u skladu s distribucijskom hipotezom da se značenje riječi određuje njezinim kontekstom.\n",
    "Word2Vec ima dvije glavne varijante:\n",
    "•\tCBOW koja predviđa središnju riječ na temelju okolnih (kontekstnih) riječi,\n",
    "•\tSKIP-GRAM koja predviđa kontekstne riječi na temelju središnje riječi.\n",
    "Vektori riječi uče se iterativnim optimizacijskim postupkom u kojem se maksimizira vjerojatnost pojave stvarnih parova riječi i konteksta u korpusu. Tijekom učenja model prilagođava vektore riječi tako da povećava sličnost vektora riječi koje se često pojavljuju zajedno, a smanjuje sličnost nepovezanih riječi.\n",
    "\n",
    "9.  Definirajte što je to TF-IDF mjera i za što se koristi.\n",
    "TF-IDF je mjera koja procjenjuje važnost neke riječi u dokumentu u odnosu na cijeli skup dokumenata (korpus).\n",
    "Sastoji se od dva dijela:\n",
    "•\tTF – mjeri koliko se često riječ pojavljuje u danom dokumentu,\n",
    "•\tIDF – umanjuje važnost riječi koje se pojavljuju u velikom broju dokumenata, a naglašava riječi koje su specifične za manji broj dokumenata.\n",
    "TF-IDF se koristi za vektorsku reprezentaciju dokumenata, isticanje informativnih riječi te u zadacima poput pretraživanja dokumenata, klasifikacije i analize teksta.\n",
    "\n",
    "10. Ako su riječi reprezentirani vektorima riječi objasnite na koji način matematički možete računati sličnost između tih riječi.\n",
    "Ako su riječi reprezentirane vektorima riječi, njihova se sličnost najčešće računa pomoću kosinusne sličnosti.\n",
    "Za dva vektora riječi ui vkosinusna sličnost definirana je kao:\n",
    "cos⁡(u,v)=(u⋅v)/(∥u∥\" \" ∥v∥)\n",
    "\n",
    "gdje je u⋅v skalarni produkt, a ∥u∥i ∥v∥euklidske norme vektora.\n",
    "Kosinusna sličnost mjeri kut između vektora, a ne njihovu duljinu, pa se riječi sličnog značenja nalaze blizu jedna drugoj u vektorskom prostoru i imaju visoku vrijednost kosinusne sličnosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "white",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
