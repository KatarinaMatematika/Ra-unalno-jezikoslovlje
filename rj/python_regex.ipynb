{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f600d95d",
   "metadata": {},
   "source": [
    "# Regularni izrazi 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fb988",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a055025",
   "metadata": {},
   "source": [
    "Osnove funkcije pretrazivanja uz pomoć python regularnih izraza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193ce89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #biblioteka za rad s regesima\n",
    "from regex import test_re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300f70f",
   "metadata": {},
   "source": [
    "## Funkcija `match()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c80afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nema podudaranja.\n"
     ]
    }
   ],
   "source": [
    "# tekst\n",
    "text = \"Mačke su pametnije od pasa.\"\n",
    "## pattern = \"Ma[cč]ke su .*\"\n",
    "pattern = \"pametnije .*\"\n",
    "\n",
    "mobj = re.match(pattern, text)\n",
    "if mobj:\n",
    "    print(\"Pronađen je podudaranje:\", mobj.group())\n",
    "else:\n",
    "    print(\"Nema podudaranja.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3b4f0",
   "metadata": {},
   "source": [
    "### Detalji o `re.match()`\n",
    "\n",
    "- `re.match(pattern, string)` pokušava pronaći podudaranje *na početku* stringa. Ako pattern odgovara od početka, vraća `Match` objekt; u suprotnom vraća `None`.\n",
    "- Najčešće korištene metode `Match` objekta:\n",
    "  - `m.group()` — vraća cijeli podudarajući tekst.\n",
    "  - `m.groups()` — vraća tuple hvatanih grupa (ako su definirane zagradama u patternu).\n",
    "  - `m.span()` — vraća tuple (start, end) pozicije podudaranja.\n",
    "- Primjer: ako želimo provjeriti počinje li tekst riječju 'Mačke', koristimo `re.match(r\"Ma[cč]ke\", text)`.\n",
    "\n",
    "Napomena: u primjeru iz bilježnice `pattern = \"pametnije .*\"` `re.match` neće pronaći podudaranje jer tekst ne počinje tim uzorkom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade6344",
   "metadata": {},
   "source": [
    "## Funkcija `search()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41983511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronađen je podudaranje: pametnije od pasa.\n"
     ]
    }
   ],
   "source": [
    "# primjer search\n",
    "text = \"Mačke su pametnije od pasa.\"\n",
    "## pattern = \"Ma[cč]ke su .*\"\n",
    "pattern = \"pametnije .*\"\n",
    "\n",
    "mobj = re.search(pattern, text)\n",
    "if mobj:\n",
    "    print(\"Pronađen je podudaranje:\", mobj.group())\n",
    "else:\n",
    "    print(\"Nema podudaranja.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6967ea8",
   "metadata": {},
   "source": [
    "### Detalji o `re.search()`\n",
    "\n",
    "- `re.search(pattern, string)` traži prvo pojavljivanje uzorka gdje god u stringu — nije ograničeno na početak.\n",
    "- Ako nađe podudaranje vraća `Match` objekt (prvo podudaranje), inače `None`.\n",
    "- Za pronalaženje svih pojavljivanja koristi se `re.finditer()` ili `re.findall()`.\n",
    "- Primjer razlike s `match`: za isti `pattern = \"pametnije .*\"` `re.search` će pronaći podudaranje u tekstu jer se ta fraza pojavljuje u sredini rečenice.\n",
    "- Korisni flagovi: `re.IGNORECASE` za ignoriranje veličine slova, `re.MULTILINE` i dr.\n",
    "\n",
    "Savjet: kad želiš testirati samo postoji li podudaranje, možeš koristiti `bool(re.search(...))`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efbd49",
   "metadata": {},
   "source": [
    "Search može traziti podudaranje u bilo kom dijelu teksta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96004eb7",
   "metadata": {},
   "source": [
    "## Funkcija `findall()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b538bbe",
   "metadata": {},
   "source": [
    "### Detalji o `re.findall()`\n",
    "\n",
    "- `re.findall(pattern, string)` vraća listu svih nepokrivenih podudaranja uzorka u tekstu.\n",
    "- Ako pattern sadrži hvatajuće grupe (zagrade), `findall` vraća listu tupleova s uhvaćenim grupama; inače vraća listu stringova.\n",
    "- Korisno kada želiš brzo izvući sve podudaranja bez iteriranja kroz `finditer()`.\n",
    "- Primjer upotrebe: traženje svih pojavljivanja određene riječi ili obrasca u tekstu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c46df002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronađena pojavljivanja: ['Mačke', 'Mačke']\n",
      "Grupe (ime, broj): [('Ivan', '3'), ('Ana', '5')]\n"
     ]
    }
   ],
   "source": [
    "# primjer findall\n",
    "text = \"Mačke su pametnije od pasa. Mačke vole mlijeko.\"\n",
    "pattern = r\"Ma[cč]ke\"\n",
    "matches = re.findall(pattern, text)\n",
    "print('Pronađena pojavljivanja:', matches)\n",
    "\n",
    "# primjer s grupama\n",
    "text2 = \"Ivan ima 3 jabuke, Ana ima 5 jabuka\"\n",
    "pattern2 = r\"(\\w+) ima (\\d+)\"\n",
    "groups = re.findall(pattern2, text2)\n",
    "print('Grupe (ime, broj):', groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e1e3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ab', 'ab']\n"
     ]
    }
   ],
   "source": [
    "matches = re.findall( 'ab', 'abbabbaab')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca735cc4",
   "metadata": {},
   "source": [
    "## Regex uzorci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21ab37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "=> Uzorak \\d+ (samo znamenke)\n",
      "'Ovo je #1 primjer. '\n",
      " ........'1'\n"
     ]
    }
   ],
   "source": [
    "# alias (specijalni znakovi)\n",
    "\n",
    "text = \"Ovo je #1 primjer. \"\n",
    "\n",
    "test_re(\n",
    "    text,\n",
    "    [\n",
    "        (r'\\d+', 'samo znamenke'),\n",
    "    ]\n",
    ")  #pronalazi znamenku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "=> Uzorak \\d+ (samo znamenke)\n",
      "'Ovo je #2 primjer. '\n",
      " ........'2'\n",
      "--------------------\n",
      "=> Uzorak \\D+ (sve osim znamenki)\n",
      "'Ovo je #2 primjer. '\n",
      " 'Ovo je #'\n",
      " .........' primjer. '\n",
      "--------------------\n",
      "=> Uzorak \\w+ (samo slova , znamenke i _)\n",
      "'Ovo je #2 primjer. '\n",
      " 'Ovo'\n",
      " ....'je'\n",
      " ........'2'\n",
      " ..........'primjer'\n"
     ]
    }
   ],
   "source": [
    "# alias (specijalni znakovi)\n",
    "\n",
    "text = \"Ovo je #2 primjer. \"\n",
    "\n",
    "test_re(\n",
    "    text,\n",
    "    [\n",
    "        (r'\\d+', 'samo znamenke'),\n",
    "        (r'\\D+', 'sve osim znamenki'),\n",
    "        (r'\\w+', 'samo slova , znamenke i _'),\n",
    "        \n",
    "    ]\n",
    ")  #pronalazi znamenku\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cb48a",
   "metadata": {},
   "source": [
    "## Klase uzorka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fec55f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "=> Uzorak [ab]+ (ili a ili b)\n",
      "'abbabbaab'\n",
      " 'abbabbaab'\n",
      "--------------------\n",
      "=> Uzorak a[ab]b+ (a iza toga a ili b barem jednom)\n",
      "'abbabbaab'\n",
      " 'abb'\n",
      " ...'abb'\n",
      " ......'aab'\n",
      "--------------------\n",
      "=> Uzorak a[ab]b+? (a iza toga a ili b barem jednom(nepohlepno))\n",
      "'abbabbaab'\n",
      " 'abb'\n",
      " ...'abb'\n",
      " ......'aab'\n"
     ]
    }
   ],
   "source": [
    "test_re(\n",
    "    'abbabbaab', # tekstni uzorak\n",
    "    [\n",
    "        (r'[ab]+', 'ili a ili b'),\n",
    "        (r'a[ab]b+', 'a iza toga a ili b barem jednom'),\n",
    "        (r'a[ab]b+?', 'a iza toga a ili b barem jednom(nepohlepno)'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625022d",
   "metadata": {},
   "source": [
    "## Kvantifikatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66cf1a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "=> Uzorak ab* (Iza a slijedi 0 ili vise b-ova)\n",
      "'abbaabbba'\n",
      " 'abb'\n",
      " ...'a'\n",
      " ....'abbb'\n",
      " ........'a'\n",
      "--------------------\n",
      "=> Uzorak a[ab]{2,4} (Iza a slijedi 2 ili 4 a ili b)\n",
      "'abbaabbba'\n",
      " 'abbaa'\n"
     ]
    }
   ],
   "source": [
    "test_re(\n",
    "    'abbaabbba',\n",
    "    [\n",
    "        (r'ab*', 'Iza a slijedi 0 ili vise b-ova' ),\n",
    "        (r'a[ab]{2,4}', 'Iza a slijedi 2 ili 4 a ili b' )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e486ce",
   "metadata": {},
   "source": [
    "## Grupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b2bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "=> Uzorak a(ab) (Iza a slijedi ab)\n",
      "'abbaaabbbbaaaaa'\n",
      " ....'aab'\n",
      "--------------------\n",
      "=> Uzorak a(a*b*) (Iza a slijedi nula ili vise a ili b)\n",
      "'abbaaabbbbaaaaa'\n",
      " 'abb'\n",
      " ...'aaabbbb'\n",
      " ..........'aaaaa'\n"
     ]
    }
   ],
   "source": [
    "test_re(\n",
    "    'abbaaabbbbaaaaa',\n",
    "    [\n",
    "        ('a(ab)', 'Iza a slijedi ab'),\n",
    "        (r'a(a*b*)', 'Iza a slijedi nula ili vise a ili b')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187b332",
   "metadata": {},
   "source": [
    "## Zadatak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f9f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ovo\n",
      "znakovima.\n",
      " tekst ?$!\n",
      " interpunkcijskim \n"
     ]
    }
   ],
   "source": [
    "text = \"Ovo je neki čudni tekst ?$! sa ---?!*$.%- interpunkcijskim znakovima.\"\n",
    "regex_uzorci = [\n",
    "    (r'\\w+', 'rijec na pocetku niza'),\n",
    "    (r'\\S+$', 'zadnja riječ s interpunkcijom'),\n",
    "    (r'\\st\\S* \\S+', 'počinje s t i sljedeča riječ'),\n",
    "    (r'\\s+\\w+m\\s+', 'završava s m')\n",
    "]\n",
    "\n",
    "for pattern, opis in regex_uzorci:\n",
    "    sobj = re.search(pattern, text)\n",
    "    print(sobj.group()) # ako postoji podudaranje ispisi ga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884c57e",
   "metadata": {},
   "source": [
    "U ovom zadatku potrebno je izgraditi n-gramski model za tekstove HR jezika koji su dani u prilogu. Tekstovi su dani u .txt formatu.\n",
    "\n",
    "Učinite sljedeće:\n",
    "  1. Izgradite skup za treniranje na kojem ćete naučiti model (80% rečenica korpusa). Evaluirajte model uz pomoć mjere perpleksnosti na skupu za testiranje (20%). Na tekstu primijenite neku od normalizacijskih  tehnika kako bi dobili što bolju perpleksnost. \n",
    "  2. Pored MLE procjenitelja, koristite barem metodu zaglađivanja i interpolacije. Odaberite onu metodu koja daje najbolju perpleksnost. \n",
    "  3. Generirajte nekoliko rečenica iz najboljeg modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ebd802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Franz Kafka\\n\\nPreobražaj\\n\\ns njemačkog preveo\\n\\nZlatk'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "with io.open('data/kafka-preobrazaj3.txt', encoding='utf8') as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8752f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import bigrams, ngrams\n",
    "from nltk.lm.models import MLE, Laplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7426534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a', 'ako', 'ali', 'bi', 'bih', 'bila', 'bili', 'bilo', 'bio', 'bismo', 'biste', 'biti', 'bumo', 'da', 'do', 'duĹľ', 'ga', 'hoÄ‡e', 'hoÄ‡emo', 'hoÄ‡ete', 'hoÄ‡eĹˇ', 'hoÄ‡u', 'i', 'iako', 'ih', 'ili', 'iz', 'ja', 'je', 'jedna', 'jedne', 'jedno', 'jer', 'jesam', 'jesi', 'jesmo', 'jest', 'jeste', 'jesu', 'jim', 'joj', 'joĹˇ', 'ju', 'kada', 'kako', 'kao', 'koja', 'koje', 'koji', 'kojima', 'koju', 'kroz', 'li', 'me', 'mene', 'meni', 'mi', 'mimo', 'moj', 'moja', 'moje', 'mu', 'na', 'nad', 'nakon', 'nam', 'nama', 'nas', 'naĹˇ', 'naĹˇa', 'naĹˇe', 'naĹˇeg', 'ne', 'nego', 'neka', 'neki', 'nekog', 'neku', 'nema', 'netko', 'neÄ‡e', 'neÄ‡emo', 'neÄ‡ete', 'neÄ‡eĹˇ', 'neÄ‡u', 'neĹˇto', 'ni', 'nije', 'nikoga', 'nikoje', 'nikoju', 'nisam', 'nisi', 'nismo', 'niste', 'nisu', 'njega', 'njegov', 'njegova', 'njegovo', 'njemu', 'njezin', 'njezina', 'njezino', 'njih', 'njihov', 'njihova', 'njihovo', 'njim', 'njima', 'njoj', 'nju', 'no', 'o', 'od', 'odmah', 'on', 'ona', 'oni', 'ono', 'ova', 'pa', 'pak', 'po', 'pod', 'pored', 'prije', 's', 'sa', 'sam', 'samo', 'se', 'sebe', 'sebi', 'si', 'smo', 'ste', 'su', 'sve', 'svi', 'svog', 'svoj', 'svoja', 'svoje', 'svom', 'ta', 'tada', 'taj', 'tako', 'te', 'tebe', 'tebi', 'ti', 'to', 'toj', 'tome', 'tu', 'tvoj', 'tvoja', 'tvoje', 'u', 'uz', 'vam', 'vama', 'vas', 'vaĹˇ', 'vaĹˇa', 'vaĹˇe', 'veÄ‡', 'vi', 'vrlo', 'za', 'zar', 'Ä‡e', 'Ä‡emo,Ä‡ete,Ä‡eĹˇ,', 'Ä‡u,', 'Ĺˇto', 'â€“', '=', '.', '..', '...', '-', '/', '\"', \"'\", ',', ':', ';', '_', '<', '+', '*', '!', '?', '%', '#', '$', '^', '&', \"''\", '``', '(', ')', '{', '}', '[', ']'\n"
     ]
    }
   ],
   "source": [
    "# ucitaj hr učestale riječi koje želimo izbaciti iz teksta\n",
    "with open('data/hr_stopwords.txt') as f:\n",
    "    stopWordsCro= f.read()\n",
    "\n",
    "print(stopWordsCro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba636f7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ketyp/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ketyp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# tokenizacija i normalizacija teksta\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m preprocess_text = [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m.lower, word_tokenize(sent))) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m      3\u001b[39m tokenized_text = []\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# filtriraj riječi koji nisu u stopWordsCro\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ketyp/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ketyp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# tokenizacija i normalizacija teksta\n",
    "preprocess_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "tokenized_text = []\n",
    "\n",
    "# filtriraj riječi koji nisu u stopWordsCro\n",
    "for i in preprocess_text:\n",
    "    tokenized_text.append([w for w in i if not w in stopWordsCro])\n",
    "\n",
    "# provjera\n",
    "tokenized_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13034764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podjela podataka na trening i test skup\n",
    "# ručna podjela podataka na 80% trening i 20% test skup\n",
    "size = int(0.8 * len(tokenized_text))\n",
    "trainSet = tokenized_text[:size]\n",
    "testSet = tokenized_text[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bfd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nadopuna n-grama sa paddingom\n",
    "n = 4\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9371df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treniranje i evaluacija modela\n",
    "mle = MLE(n)\n",
    "mle.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daced0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kolika je perpleksnost modela?\n",
    "mle.perplexity(testSet) # inf jer imamo n-grame koji nisu vidjeni u trening skupu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace-ov model\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trainSet)\n",
    "\n",
    "laplace = Laplace(n)\n",
    "laplace.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d24464",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace.perplexity(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generiranje rečenica\n",
    "# pretvori listu tokena u rečenicu\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return TreebankWordDetokenizer().detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sent(laplace, 10, random_seed=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
