{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb07e732",
   "metadata": {},
   "source": [
    "# V1:Python string parsiranje\n",
    "\n",
    "Što čemo naučiti?\n",
    "  * pisati jupiter bilježnicu u Pythonu i markdown jeziku\n",
    "  * ponoviti osnovne strukture podataka iz pythona\n",
    "  * rad sa Python stringovima\n",
    "  * dolazenje do tekstualnih informacija struganjem web-a (web scraping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bba231",
   "metadata": {},
   "source": [
    "## 1.1 Tipovi podataka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b473e03",
   "metadata": {},
   "source": [
    "Ponovimo ukratko Python tipove podataka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63afb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4556a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'complex'>\n",
      "<class 'str'>\n",
      "<class 'int'>\n",
      "<class 'complex'>\n",
      "<class 'set'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# tipovi podataka\n",
    "x = [3+4j, '3+4j', 3,4j, {3+4j}, \"3+4j\", 4.3, [3,5], [4j], (1,2)]\n",
    "for i in x:\n",
    "    print(type(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc77e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ana', 'ima', 'jabuku,', 'Marko', 'ima', 'krušku.']\n",
      "['Ana ima jabuku', ' Marko ima krušku.']\n",
      "Ana  | ima |  jabuku, Marko ima krušku.\n",
      "Ana ima ja\n"
     ]
    }
   ],
   "source": [
    "# Primjeri razdvajanja stringova (split, partition, slicing)\n",
    "tekst = 'Ana ima jabuku, Marko ima krušku.'\n",
    "\n",
    "# Split po razmaku\n",
    "rijeci = tekst.split()\n",
    "print(rijeci)\n",
    "\n",
    "# Split po zarezu\n",
    "dijelovi = tekst.split(',')\n",
    "print(dijelovi)\n",
    "\n",
    "# Partition po riječi 'ima'\n",
    "prvi, sep, ostatak = tekst.partition('ima')\n",
    "print(prvi, '|', sep, '|', ostatak)\n",
    "\n",
    "# Slicing: prvi dio do 10. znaka\n",
    "print(tekst[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e9806",
   "metadata": {},
   "source": [
    "## Strukture podataka u Pythonu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951e48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista: [1, 2, 3, 4]\n",
      "Tuple: (1, 2, 3, 4)\n",
      "Set: {1, 2, 3}\n",
      "Rječnik: {'ime': 'Ana', 'godine': 25}\n"
     ]
    }
   ],
   "source": [
    "# Primjeri osnovnih struktura podataka u Pythonu (osim stringova)\n",
    "\n",
    "# Lista\n",
    "lista = [1, 2, 3, 4]\n",
    "print('Lista:', lista)\n",
    "\n",
    "# Tuple\n",
    "torka = (1, 2, 3, 4)\n",
    "print('Tuple:', torka)\n",
    "\n",
    "# Set\n",
    "skup = {1, 2, 2, 3}\n",
    "print('Set:', skup)\n",
    "\n",
    "# Rječnik (dict)\n",
    "rjecnik = {'ime': 'Ana', 'godine': 25}\n",
    "print('Rječnik:', rjecnik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46acc8a",
   "metadata": {},
   "source": [
    "**Objektno orijentirano programiranje (OOP)** je način programiranja gdje se program sastoji od objekata. Objekti su kombinacija podataka (atributa) i funkcija (metoda) koje rade s tim podacima. OOP olakšava organizaciju i ponovnu upotrebu koda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install nltk scikit-learn numpy pandas matplotlib -y # anaconda python 3 \n",
    "!pip install nltk scikit-learn numpy pandas matplotlib # osnovni python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3259c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # preuzeti podatke za nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786061ee",
   "metadata": {},
   "source": [
    "U mapi `data\\rjecnik.txt` dano vam je popis riječi sa gramatičkim i semantičkim obilježjima.  Iz teksta izvući samo imenice sa opisom i gramatičkim obilježjima i spremiti u JSON datoteku prema sljedećem formatu\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('data/ocr.txt', 'r', encoding='utf8') as ocr:\n",
    "    content = ocr.read()\n",
    "\n",
    "    entries = re.split(r'\\n\\n', content)\n",
    "\n",
    "    regex = r'(?P<lemma>\\w+)\\s+(?P<pos>im.)\\s+(?P<gender>.*)\\s+〈(?P<inflection>.*)〉(?P<definition>.*)'    \n",
    "\n",
    "    for i,data in enumerate(entries):\n",
    "        print(f'\\n\\npodatak {i}: ', data) \n",
    "        lex = {}\n",
    "        mObj = re.match(regex, data, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "\n",
    "        if mObj:\n",
    "            print('\\n**Pronasao uzorak: ', end=' ')\n",
    "            print(mObj.groupdict())\n",
    "            lex['lemma'], lex['pos'], lex['gender'], lex['inflection'], lex['definition'] = mObj.group('lemma'), mObj.group(\"pos\"), mObj.group(\"gender\"), mObj.group(\"inflection\"), mObj.group(\"definition\")\n",
    "            \n",
    "            jsonObj = json.dumps(lex, ensure_ascii=False, indent=4)\n",
    "\n",
    "            with open(f\"data/{lex['lemma']}.json\", \"w\", encoding='utf8') as outfile:\n",
    "                outfile.write(jsonObj)\n",
    "        else: \n",
    "            print('Nije imenica')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93918350",
   "metadata": {},
   "source": [
    "prosla. 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a822b2",
   "metadata": {},
   "source": [
    "U prilogu vam da je dan izvadak iz Školskog rječnika hrvatskog jezika. Vaš je zadatak sljedeći:\n",
    "  * Izvući glagole s gramatičkim obilježjima te opisom kako je dano u primjeru `gakati.json`\n",
    "  * izvući pridjeve s gramatičkim obilježjima kako je dano u primjeru: `gadan.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945210db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "with io.open('SK_rjecnik.txt', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    " \n",
    " \n",
    "text = text.replace('\\t', '')\n",
    " \n",
    "import re, json\n",
    "import unidecode\n",
    " \n",
    "entries = re.split('\\n\\n', text)\n",
    " \n",
    "for i in range(len(entries)):\n",
    "    entries[i] = entries[i].replace('\\n', ' ')\n",
    "entries\n",
    " \n",
    "regex = '((\\w+)\\s+(gl.)\\s+(nesvrš.|svrš.)\\s+(neprijel.|prijel.|prijel./neprijel.)\\s+(〈.*?〉))'\n",
    " \n",
    "for data in entries:\n",
    "    lex = {}\n",
    "    mObj = re.match(regex, data, re.MULTILINE | re.DOTALL)\n",
    "    if mObj:\n",
    "        lex['rijec'], lex['vrsta'], lex['vid'], lex['prijelaznost'], lex['gramaticka obiljezja'] = unidecode.unidecode(mObj.group(2)), unidecode.unidecode(mObj.group(3)), unidecode.unidecode(mObj.group(4)), unidecode.unidecode(mObj.group(5)), unidecode.unidecode(mObj.group(6))\n",
    "        lex['definicija'] = data.partition(mObj.group(1))[2]\n",
    "       \n",
    "        jsonObj = json.dumps(lex)\n",
    " \n",
    "        with open(f\"{lex['rijec']}.json\", 'w') as outfile:\n",
    "            outfile.write(jsonObj)      \n",
    " \n",
    " \n",
    "         \n",
    "entries = re.split('\\n\\n', text)\n",
    " \n",
    "for i in range(len(entries)):\n",
    "    entries[i] = entries[i].replace('\\n', ' ')\n",
    "entries\n",
    "regex2 = '((\\w+)\\s+(prid.)\\s+(〈.*?〉))'\n",
    " \n",
    "for data2 in entries:\n",
    "    lex2 = {}\n",
    "    mObj = re.match(regex2, data2, re.MULTILINE | re.DOTALL)\n",
    "    if mObj:\n",
    "        lex2['rijec'], lex2['vrsta'], lex2['gramaticka obiljezja'] = unidecode.unidecode(mObj.group(2)), unidecode.unidecode(mObj.group(3)), unidecode.unidecode(mObj.group(4))\n",
    "        lex2['definicija'] = data2.partition(mObj.group(1))[2]\n",
    "       \n",
    "        jsonObj = json.dumps(lex2)\n",
    " \n",
    "        with open(f\"{lex2['rijec']}.json\", 'w') as outfile:\n",
    "            outfile.write(jsonObj)\n",
    "entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bdd3d",
   "metadata": {},
   "source": [
    "2.Implementirajte analizu sentimenta koristeći naivni Bayes za popis filmskih recenzija koje se nalaze u `nltk.corpus.movie_reviews`. Za značajke pojedine recenzije koristite informaciju sadrži li recenzija najčešćih $K=2000$ riječi iz `movie_reviews` korpusa. Točnost (accuracy) klasifikatora mora biti barem 60%. Prikažite mjere preciznosti, odziva i $F_1$ ocjenu za pojedinu kategoriju pozitivnog i negativnog sentimenta te prikažite amtricu zbunjenosti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaše rješenje ...\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    " \n",
    "nltk.download('movie_reviews')\n",
    " \n",
    "all_words = [word.lower() for word in movie_reviews.words()]\n",
    "freq_dist = FreqDist(all_words)\n",
    "most_common_words = [word for word, _ in freq_dist.most_common(2000)]\n",
    " \n",
    "\n",
    "def extract_features(review_words):\n",
    "    review_words_set = set(review_words)\n",
    "    features = {word: (word in review_words_set) for word in most_common_words}\n",
    "    return features\n",
    " \n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    " \n",
    "# Miješanje - nasumično raspoređivanje\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(documents)\n",
    "\n",
    "feature_sets = [(extract_features(words), category) for words, category in documents]\n",
    "train_set, test_set = feature_sets[:1600], feature_sets[1600:]\n",
    " \n",
    "# Treniranje naivnog Bayesovog klasifikatora\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "# Evaluacija modela\n",
    "print(\"Točnost klasifikatora:\", accuracy(classifier, test_set) * 100)\n",
    "\n",
    " \n",
    "# Prikaz najvažnijih značajki\n",
    "classifier.show_most_informative_features(10)\n",
    " \n",
    "# Predikcije - testni skup\n",
    "y_true = [label for _, label in test_set]\n",
    "y_pred = [classifier.classify(features) for features, _ in test_set]\n",
    " \n",
    "# Metričke evaluacije\n",
    "print(\"\\nIzvještaj o klasifikaciji:\")\n",
    "print(classification_report(y_true, y_pred, target_names=movie_reviews.categories()))\n",
    " \n",
    "print(\"\\nMatrica zbunjenosti:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced8e17",
   "metadata": {},
   "source": [
    "3. Učinite sljedeće:\n",
    " 1. Izračunajte TF-IDF vektor za svaku rečenicu u dokumentu. Prikazati rezultirajuće vektore za svaku rečenicu.\n",
    "\n",
    " 2. Primijenite K-Means algoritam nad dobivenim TF-IDF vektorima dokumenata. Pretpostavite K = 3. Ispišite tablično kojem klasteru pripada koja rečenica.\n",
    "\n",
    " 3. Dobili ste sljedeću rečenicu: `Reinforcement learning is used also in natural language processing` Pronađite kojem klasteru ova rečenica pripada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaše rješenje\n",
    "from sklearn.feature_extraction.text import CountVectorizer # pretvara dokumente u vektore frekvencija tokena\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # \n",
    "\n",
    "# NLTK funkcije\n",
    "from nltk.stem.porter import PorterStemmer # korijenovatelj engleskih riječi\n",
    "from sklearn.cluster  import KMeans   # algoritam klasteriranja\n",
    "import pandas as pd\n",
    "\n",
    "sents = ['Machine learning algorithms use data to make predictions.','Deep learning models require large amounts of labeled data.','Natural language processing techniques analyze textual data.','Milena came home after finishing her workout, immediately took off her backpack, and washed her hands.','She sat down at the table to eat.','Then she focused on her homework, not thinking about tomorrow’s match.','How can you accentuate words in English?','Do you want to learn a new language quickly and efficiently?','Exploring English syntax: embark on an adventure through English sentence structure!']\n",
    "print(sents)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "# tokeniziraj i prebroj\n",
    "X = count_vectorizer.fit_transform(sents)\n",
    "# koristi panadas za prikaz\n",
    "#pd.DataFrame(X.toarray())\n",
    "# poboljsaj prikaz?\n",
    "pd.DataFrame(X.toarray(),columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# novi vektorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "X = count_vectorizer.fit_transform(sents)\n",
    "pd.DataFrame(X.toarray(),columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# lematizator \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# tokenizator\n",
    "def tokenizer(text):\n",
    "    words = re.sub(r'[^A-Za-z0-9\\-]',\" \",text).lower().split() # prave riječi u tekstu\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# vektorizator\n",
    "count_vectorizer = CountVectorizer(stop_words='english',tokenizer=tokenizer)\n",
    "X = count_vectorizer.fit_transform(sents)\n",
    "pd.DataFrame(X.toarray(),columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, use_idf=False, norm='l1')\n",
    "X = tfidf_vectorizer.fit_transform(sents)\n",
    "df=pd.DataFrame(X.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(sents)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16613e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algoritam klasteriranja\n",
    "from sklearn.cluster import KMeans\n",
    "# broj klastera 3\n",
    "number_of_clusters = 3\n",
    "km = KMeans(n_clusters=number_of_clusters)\n",
    "km.fit(X);vectorizer = TfidfVectorizer(use_idf=True, tokenizer=tokenizer, stop_words='english')\n",
    "X = vectorizer.fit_transform(sents)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['text'] = sents\n",
    "results['category'] = km.labels_ # klaster oznake\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['Machine learning algorithms use data to make predictions.','Deep learning models require large amounts of labeled data.','Natural language processing techniques analyze textual data.','Milena came home after finishing her workout, immediately took off her backpack, and washed her hands.','She sat down at the table to eat.','Then she focused on her homework, not thinking about tomorrow’s match.','How can you accentuate words in English?','Do you want to learn a new language quickly and efficiently?','Exploring English syntax: embark on an adventure through English sentence structure!']\n",
    "print(sents)\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer = TfidfVectorizer(use_idf=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(sents)\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# broj klastera\n",
    "number_of_clusters = 3\n",
    "km = KMeans(n_clusters=number_of_clusters)\n",
    "km.fit(X)\n",
    "\n",
    "# ispisi rezultat\n",
    "results = pd.DataFrame()\n",
    "results['text'] = sents\n",
    "results['category'] = km.labels_\n",
    "results\n",
    "\n",
    "# novi podatak\n",
    "text = ['Reinforcement learning is used also in natural language processing']\n",
    "x = vectorizer.transform(text)\n",
    "#  predicted cluster\n",
    "predicted_cluster = km.predict(x)\n",
    "\n",
    "\n",
    "print(f\"'{text[0]}' belongs to cluster {predicted_cluster[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
